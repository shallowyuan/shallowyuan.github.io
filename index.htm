<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">		
		
		<meta charset="utf-8">
		
		<!-- ADD THE PAGE TITLE BELOW -->
		<title>Zehuan Yuan(袁泽寰)</title>
		
		<link rel="stylesheet" href="./zehuan/style.css">
	  
<script type="text/javascript">window.onerror=function(){return true;}</script></head>
	
	<body>	
		
		<div id="wrap">
		
			<div id="header">
				<div id="header-content">					
		
					<div id="AKA">
					<h1>Zehuan Yuan</h1>
					</div>
					<h2></h2> 
				    <p>
					I am currently a Tech Manager in <a href="https://www.bytedance.com/en/">Bytedance Inc.</a> Before that, I received both B.S. degree  and Ph.D. degree from  <a href="http://cs.nju.edu.cn/">Department of computer science and technology</a>,
					<a href="http://nju.edu.cn/"> Nanjing University</a>, under the supervision of Professor <a href="https://cs.nju.edu.cn/lutong/index.htm"> Tong Lu</a> . During my PhD study, I was lucky to visit the <a href="https://www.umich.edu"> University of Michigan</a> working with <a href="https://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a> for one and a half year. 
					</p>
					<br>
					<p>
		            My interests lie in performing foundamental generative AI researach, building interesting and powerful AIGC capabilities, and applying these techs into real business scenarios. You can find and connect me from  <a href="https://scholar.google.com/citations?user=FqMOHnEAAAAJ&hl=zh-CN"> <font color="blue">Google Scholar</font></a> and <a href="https://www.linkedin.com/in/zehuan-yuan-40961182/"> <font color="blue">Linkedin</font></a>.
					</p>
					<br>
					<p><b>[<font color="red">Hiring!</font>]</b> Actively recruiting research scientists, algorithm engineers and research interns in China and Singapore. Feel free to contact me!</p>
					<p>
						<a href="https://job.toutiao.com/s/i2Svs7sG">[研究科学家]</a> <a href="https://job.toutiao.com/s/i2SvMX5X">[算法工程师]</a> <a href="https://job.toutiao.com/s/i2SoHFQV">[研究实习生]</a>
					</p>
					<div id="contact-details">	
						<p class="contact"><b>[<font color="red">Contact</font>]</b> <a href="mailto:%20yuanzehuan@bytedance.com">yuanzehuan@bytedance.com
						</a> </p>
					</div>	
									
				</div><!--end header-content-->
				
				<div id="header-photo">
						
					<!-- ADD A URL TO YOUR PHOTO HERE - 200x200px IN SIZE -->		
					<img src="./zehuan/my.jpeg">
					
				</div><!--end header-photo-->
			
			</div><!--end header-->
			
			<div id="content">			
				<div id="experience" class="cv-section">

					<div class="line"></div>
					<h3> News</h3>

						<li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							We release a strong video generation model <a href="https://github.com/FoundationVision/Waver?tab=readme-ov-file"> Waver </a> that ranks <font color="red"><b>Top3</b></font> in the <a href="https://artificialanalysis.ai/text-to-video/arena?tab=leaderboard-image"> Artificial Analysis Arena </a>
						</li>

					    <li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							Our text-to-image model <a href="https://foundationvision.github.io/infinity.project/">Infinity</a> based on VAR is available now with code and demo. 
						</li>
					    <li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							Our paper <a href="https://github.com/FoundationVision/VAR"> VAR </a> wins the <font color="red"><b> Best Paper Award </b> </font> in NeurIPS 2024.
						</li>
					    <li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							We propose a new image generation  paradigm <a href="https://github.com/FoundationVision/VAR"> VAR </a>|<a href="https://www.jiqizhixin.com/articles/2024-04-15-5?from=synced&keyword=VAR"> Report. </a>
						</li>
						<li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							15 papers are accepted in 2023y (PAMI 3, CVPR 4, ICCV 3 , ICLR 2, NeurIPS 1, and 2 papers in SIGIR and ACM MM)
						</li>
					    <li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							Our <a href="https://github.com/ifzhang/ByteTrack">ByteTrack</a> <b> ranks <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/"> 1th </a> of the most influential papers in ECCV 2022.</b>
						</li>
						<li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							15 papers are accepted in 2022y (TIP 2, CVPR 3, ECCV 5 , NeurIPS 3, and 2 papers in ICLR and AAAI)
						</li>
						<li style="list-style-type:square; margin-left:20px; margin-bottom:8px">
							Our global team wins <b> the first prize in the Trusted Media Challenge (TMC) combatting deepfakes </b><a href="https://jobs.bytedance.com/en/blog/7092969057356728584?spread=XE4EC8K">[report]</a>
						</li>

					    <li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							Sparse R-CNN is accepted by CVPR 2021 and integrated by Detectron2, MMDetection and PaddlePaddle.
						</li>
						<li style="list-style-type:square; margin-left:20px; margin-bottom:8px; display:list-item">
							Our paper <a href="https://arxiv.org/abs/2004.00917">Controllable Orthogonalization</a> is selected in <font color="red"><b>Best Paper Award Nominees </b></font> by CVPR 2020. 
						</li>

					<br>
					<div class="line"></div>
				 	<h3>Selected Publications [<a href="https://scholar.google.com/citations?user=FqMOHnEAAAAJ&hl=zh-CN">Full List</a>]</h3>
				 	
					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/logo.png">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title"> Waver: Wave Your Way to Lifelike Video Generationn></h4>
	                       <h4 class="meta">Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng,  <font color=#333> Zehuan Yuan</font> </h4>
	                       </a><h4 class="meta"> </font> <font color="red" ><b>Rank top3 in T2V & I2V arena leaderboard</b></font></h4>
	                       <a href="https://arxiv.org/abs/2508.15761">[arxiv paper]
	                       <a href="https://github.com/FoundationVision/Waver?tab=readme-ov-file">[project]</a>
						</div>	
					</div>

					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/hllm.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">HLLM: Enhancing sequential recommendations via hierarchical large language models for item and user modeling</h4>
	                       <h4 class="meta">Junyi Chen, Lu Chi, Bingyue Peng, <font color=#333> Zehuan Yuan</font></h4>
	                       <a href="https://arxiv.org/pdf/2409.12740">[arxiv paper]</a>
	                       <a href="https://github.com/bytedance/HLLM">[code]</a>
						</div>	
					</div>

					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/infinity.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis</h4>
	                       <h4 class="meta">Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, <font color=#333> Zehuan Yuan</font>,  Bingyue Peng, Xiaobing Liu </h4>
	                       <h4 class="meta"> <font color=#333> CVPR 2025</font> <font color="red" > Official VAR Text2Image Model</font></h4>
	                       <a href="https://arxiv.org/pdf/2412.04431">[paper]</a>
	                       <a href="https://foundationvision.github.io/infinity.project/">[project]</a>
						</div>	
					</div>

					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/goku.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Goku: Flow Based Video Generative Foundation Models</h4>
	                       <h4 class="meta">Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, <font color=#333> Zehuan Yuan</font>,  Bingyue Peng, Xiaobing Liu </h4>
	                       	<h4 class="meta"> <font color=#333> CVPR 2025</font></h4>
	                       <a href="https://arxiv.org/abs/2502.04896">[paper]</a>
	                       <a href="https://saiyan-world.github.io/goku/">[project]</a>
						</div>	
					</div>

					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/tokenflow.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Tokenflow: Unified image tokenizer for multimodal understanding and generation</h4>
	                       <h4 class="meta">Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K Du, <font color=#333> Zehuan Yuan</font>, Xinglong Wu </h4>
	                       	<h4 class="meta"> <font color=#333> CVPR 2025</font></h4>
	                       <a href="https://arxiv.org/pdf/2412.03069">[paper]</a>
	                       <a href="https://github.com/ByteFlow-AI/TokenFlow?tab=readme-ov-file/">[code]</a>
						</div>	
					</div>

					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/var.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Visual autoregressive modeling: Scalable image generation via next-scale prediction</h4>
	                       <h4 class="meta">Keyu Tian, Yi Jiang, <font color=#333>Zehuan Yuan</font>, Bingyue Peng, Liwei Wang</h4>
	                       <h4 class="meta"> <font color=#333> NeurIPS 2024</font><font color="red" > <b>[Best Paper Award]</b></font></h4>
	                       <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/9a24e284b187f662681440ba15c416fb-Paper-Conference.pdf">[paper]</a>
	                       <a href="https://github.com/FoundationVision/VAR">[code]</a>
						</div>	
					</div>

					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/llamagen.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Autoregressive model beats diffusion: Llama for scalable image generation</h4>
	                       <h4 class="meta">Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, <font color=#333>Zehuan Yuan</font></h4>
	                       <a href="https://arxiv.org/abs/2406.06525">[arxiv paper]</a>
	                       <a href="https://github.com/FoundationVision/LlamaGen">[code]</a>
						</div>	
					</div>
					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/groma.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Groma: Localized visual tokenization for grounding multimodal large language models</h4>
	                       <h4 class="meta">Chuofan Ma, Yi Jiang, Jiannan Wu, <font color=#333>Zehuan Yuan</font>, Xiaojuan Qi</h4>
	                       <h4 class="meta"> <font color=#333> ECCV 2024</font></h4>
	                       <a href="https://arxiv.org/pdf/2404.13013?">[paper]</a>
	                       <a href="https://github.com/FoundationVision/Groma">[code]</a>
						</div>	
					</div>
					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/sparse.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Sparse r-cnn: End-to-end object detection with learnable proposals</h4>
	                       <h4 class="meta">Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, <font color=#333> Zehuan Yuan</font>, Changhu Wang, Ping Luo</h4>
	                       <h4 class="meta"> <font color=#333> TPAMI 2023</font></h4>
	                       <a href="https://arxiv.org/abs/2011.12450">[paper]</a>
	                       <a href="https://github.com/PeizeSun/SparseR-CNN">[code]</a>
						</div>	
					</div>
					<!-- 这是一个 HTML 注释，用于说明文档的标题 -->
					
					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/bytetrack.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Bytetrack: Multi-object tracking by associating every detection box</h4>
	                       <h4 class="meta">Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, <font color=#333> Zehuan Yuan</font>, Ping Luo, Wenyu Liu, Xinggang Wang</h4>
	                       <h4 class="meta"> <font color=#333> ECCV 2022</font> <font color="red" >The most influential paper in ECCV 2022 (rank 1th)</font></h4>
	                       <a href="https://arxiv.org/pdf/2110.06864">[paper]</a>
	                       <a href="https://github.com/ifzhang/ByteTrack">[code]</a>
						</div>	
					</div>

					<!-- 这是一个 HTML 注释，用于说明文档的标题 -->
					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/transtrack.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Transtrack: Multiple object tracking with transformer</h4>
	                       <h4 class="meta">Peize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, <font color=#333>Zehuan Yuan</font>, Changhu Wang, Ping Luo</h4>
	                       <a href="https://arxiv.org/pdf/2012.15460">[arxiv paper]</a>
	                       <a href="https://github.com/PeizeSun/TransTrack">[code]</a>
						</div>	
					</div>
					<div class="cv-section-item">
	                   <div class="cv_image">
	                   		<img src="./zehuan/focal.jpg">
	                   </div>
	                    <div class="cv_content">
	                       <h4 class="title">Focal and global knowledge distillation for detectors</h4>
	                       <h4 class="meta">Zhendong Yang, Zhe Li, Xiaohu Jiang, Yuan Gong, <font color=#333>Zehuan Yuan</font>, Danpei Zhao, Chun Yuan</h4>
	                       <h4 class="meta"> <font color=#333> CVPR 2022</font></h4>
	                       <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Focal_and_Global_Knowledge_Distillation_for_Detectors_CVPR_2022_paper.pdf">[paper]</a>
						</div>	
					</div>
					<!-- 这是一个 HTML 注释，用于说明文档的标题 -->
				</div>
			</div>
		</div>
	</body>
</html>